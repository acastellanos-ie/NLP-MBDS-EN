{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/NLP-MBDS-EN/blob/main/07_rag/RAG_practice_step_by_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Implementing a Step-by-Step RAG Practice with LangChain\n",
        "\n",
        "Welcome to this interactive notebook where we will build a **Retrieval-Augmented Generation (RAG)** system!\n",
        "\n",
        "In previous practices, we explored Extractive Question Answering and standalone Large Language Models (LLMs) like LLaMA-2 acting as chatbots. While powerful, LLMs have a crucial limitation: they are prone to **hallucinations** (inventing facts) and lack access to private or recent data not seen during their training.\n",
        "\n",
        "**RAG** solves this issue by combining two components:\n",
        "1.  **Retrieval Component**: Searches a custom knowledge base (like your own PDF documents, databases, or websites) for relevant information based on a user's question.\n",
        "2.  **Generation Component**: A powerful LLM takes the retrieved information as \"context\" and uses it to formulate a precise, well-reasoned answer.\n",
        "\n",
        "To build this efficiently, we will use **[LangChain](https://python.langchain.com/)**, a state-of-the-art framework designed specifically to make building applications powered by LLMs a breeze.\n",
        "\n",
        "### In this notebook, we will:\n",
        "- Set up the environment and install necessary libraries.\n",
        "- **Step 1**: Load and chunk a custom document to create our Knowledge Base.\n",
        "- **Step 2**: Create vector Embeddings and a Vector Store (FAISS) for lightning-fast retrieval.\n",
        "- **Step 3**: Initialize an efficient generative LLM using 4-bit Quantization (to run fast on free hardware).\n",
        "- **Step 4**: Assemble the RetrievalQA Chain using LangChain.\n",
        "- **Step 5**: Map our fully functional RAG app to a beautiful interactive Web UI using Gradio!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu_reminder"
      },
      "source": [
        "Ensure that you have the **GPU runtime** activated:\n",
        "(Runtime -> Change runtime type -> Hardware accelerator -> GPU (T4 is perfect))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_title"
      },
      "source": [
        "## Setup: Installing Dependencies\n",
        "\n",
        "Let's install all the specialized tools we need. This includes LangChain components, HuggingFace transformers, FAISS (vector DB), and Gradio (UI).\n",
        "\n",
        "*Note: We are installing `bitsandbytes` and `accelerate` to load the LLM efficiently using quantization.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_code"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq langchain langchain-community langchain-huggingface langchain-text-splitters\n",
        "!pip install -Uqqq sentence-transformers faiss-cpu beautifulsoup4\n",
        "!pip install -Uqqq transformers accelerate bitsandbytes\n",
        "!pip install -Uqqq gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_title"
      },
      "source": [
        "## Step 1: Document Loading and Chunking\n",
        "\n",
        "To build our custom knowledge base, we need a document. For this example, let's scrape a Wikipedia article using LangChain's handy `WebBaseLoader`.\n",
        "\n",
        "**The Rationale (Why Chunk?):** LLMs have a strict limit on how much text they can process at once, known as the **context window**. If we pass an entire Wikipedia page to the LLM, it will likely crash or forget the beginning. To solve this, we must split our long document into smaller, bite-sized pieces called **chunks**.\n",
        "\n",
        "For this practice, we use `RecursiveCharacterTextSplitter`. Notice the `chunk_overlap` parameter? We use an overlap so that if a sentence is split midway, the next chunk will contain the preceding words, preventing loss of context!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step1_code"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 1. Load the document (You can change this URL to any article you like!)\n",
        "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "loader = WebBaseLoader(url)\n",
        "data = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(data)} document(s).\")\n",
        "print(f\"Original character count: {len(data[0].page_content)}\")\n",
        "\n",
        "# 2. Split the document into chunks\n",
        "# We use RecursiveCharacterTextSplitter which tries to keep paragraphs and sentences together.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,   # Maximum size of each chunk\n",
        "    chunk_overlap=150, # Overlap helps prevent cutting the context mid-sentence\n",
        "    add_start_index=True\n",
        ")\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "print(f\"\\nSplit into {len(docs)} chunks.\")\n",
        "print(f\"Example Chunk:\\n{docs[10].page_content[:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_title"
      },
      "source": [
        "## Step 2: Embeddings and Vector Store (The Retriever)\n",
        "\n",
        "Now we have our text chunks. How do we quickly search through them when a user asks a question?\n",
        "\n",
        "**The Rationale (What are Embeddings?):** We use an **Embedding Model** to convert text into fixed-size numbers (vectors). Think of it as mapping sentences on a 3D graph: texts with similar meanings (e.g., 'dog' and 'puppy') end up grouped closely together. \n",
        "\n",
        "By storing these vectors in a **Vector Store** (like FAISS), our system can mathematically compare a user's question to thousands of chunks and instantly return the most relevant ones. No need to keyword-match!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step2_code"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# 1. Select the embedding model\n",
        "# all-MiniLM-L6-v2 is an excellent, compact embedding model built by SentenceTransformers\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# 2. Create the FAISS Vector Index\n",
        "# This processes all 'docs' through the embedding model and builds the searchable database\n",
        "print(\"Generating embeddings and indexing into FAISS. This may take a minute...\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Create the Retrieval interface\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Retrieve the top 3 most relevant chunks\n",
        "print(\"Indexing Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HwH4BAsYmvw"
      },
      "outputs": [],
      "source": [
        "# Let's test the retriever standalone!\n",
        "test_query = \"What is machine learning?\"\n",
        "relevant_docs = retriever.invoke(test_query)\n",
        "print(f\"\\nRetrieved {len(relevant_docs)} docs for the query '{test_query}'.\")\n",
        "\n",
        "for doc in relevant_docs:\n",
        "  print(\"\\n---\")\n",
        "  print(\"Content: \", doc.page_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_title"
      },
      "source": [
        "## Step 3: Generator Setup (The LLM)\n",
        "\n",
        "**The Rationale (The Brain of RAG):** The Generator is the 'brain' of our system. It reads the retrieved chunks (context) and writes a fluid, human-like answer.\n",
        "\n",
        "We are using an open, lightweight model called **`TinyLlama/TinyLlama-1.1B-Chat-v1.0`**. We chose this because loading massive models like LLaMA-2 7B directly into a free Colab GPU might crash due to lack of RAM.\n",
        "\n",
        "**What is Quantization?** To make the model run even faster and use less memory, we use a technique called **4-bit Quantization** via the `bitsandbytes` library. It compresses the model's 'weights' (its internal math) so it fits perfectly on a modest GPU without losing much intelligence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step3_code"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Configuration for 4-bit Quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(f\"Loading model tokenizer and weights ({model_id})...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\" # Automatically maps to GPU if available\n",
        ")\n",
        "\n",
        "# Build the HuggingFace Generation Pipeline\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=0.1,    # We keep temperature low in RAG to avoid hallucinations\n",
        "    max_new_tokens=256, # Max length of the answer it generates\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False # We only want the generated answer, not the prompt echoed back\n",
        ")\n",
        "\n",
        "# Wrap the pipeline so LangChain can converse with it\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "print(\"LLM loaded and pipeline wrapped!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_title"
      },
      "source": [
        "## Step 4: Putting It All Together (The RAG Chain)\n",
        "\n",
        "We have our Retriever (FAISS) and our Generator (TinyLlama). Now we use LangChain to wire them together.\n",
        "\n",
        "We'll define a **Prompt Template** that instructs the LLM:\n",
        "\"Here is some context. Use it to answer the question. If you don't know the answer, just say you don't know.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step4_code"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "# 1. Define the Prompt exactly as the LLM expects it\n",
        "prompt_template = \"\"\"<|system|>\n",
        "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "\n",
        "Context: {context}</s>\n",
        "<|user|>\n",
        "{input}</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# 2. Build the Document Chain\n",
        "# This simply tells LangChain: \"Take the prompt, and 'stuff' all the retrieved documents into the {context} variable.\"\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# 3. Build the actual RAG Retrieval Chain\n",
        "# This wires the Retriever (FAISS) and the Document Chain (LLM) together seamlessly.\n",
        "rag_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "print(\"RAG Pipeline is ready! Notice how simple LangChain makes this integration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4.5_title"
      },
      "source": [
        "Let's test the RAG Chain programmatically to see if everything works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step4.5_code"
      },
      "outputs": [],
      "source": [
        "user_question = \"Who formulated the concept of weak AI and strong AI?\"\n",
        "\n",
        "# The chain expects the question to be passed via an 'input' dictionary\n",
        "result = rag_chain.invoke({\"input\": user_question})\n",
        "\n",
        "print(\"QUESTION:\", user_question)\n",
        "print(\"\\n--- LLM ANSWER ---\")\n",
        "print(result[\"answer\"])\n",
        "\n",
        "print(\"\\n--- CITED SOURCES (Context) ---\")\n",
        "# Because we used create_retrieval_chain, LangChain automatically attaches the original documents \n",
        "# used to formulate the answer under the 'context' key!\n",
        "for i, doc in enumerate(result['context'], 1):\n",
        "    content_preview = doc.page_content.replace(\"\\n\", \" \")[:150]\n",
        "    print(f\"Source {i} snippet: {content_preview}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_title"
      },
      "source": [
        "## Step 5: Interactive Chat UI with Gradio\n",
        "\n",
        "Testing with Python output is great for developers, but applications are built for end-users. We'll wrap our LangChain logic in a `Gradio` Web UI.\n",
        "\n",
        "We define a helper function (`chat_with_rag`) that Gradio will trigger every time the user clicks submit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step5_code"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_with_rag(message, history):\n",
        "    # Using our rag_chain to generate a response\n",
        "    response = rag_chain.invoke({\"input\": message})\n",
        "    \n",
        "    # We fetch the answer string from the output dictionary\n",
        "    answer = response[\"answer\"]\n",
        "    return answer.strip()\n",
        "\n",
        "# Create an aesthetic UI with Gradio Themes\n",
        "custom_theme = gr.themes.Ocean(primary_hue=\"blue\", neutral_hue=\"slate\")\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_rag,\n",
        "    title=\"üß† My RAG Knowledge Assistant\",\n",
        "    description=\"Ask me anything about Artificial Intelligence! My answers are generated by **TinyLlama** using strictly the provided **Wikipedia Knowledge Base**.\",\n",
        "    examples=[\n",
        "        \"What is the Turing test?\", \n",
        "        \"Who are the pioneers of AI?\", \n",
        "        \"Explain deep learning briefly.\"\n",
        "    ],\n",
        "    theme=custom_theme,\n",
        "    submit_btn=\"Send üöÄ\",\n",
        "    retry_btn=\"Retry üîÑ\",\n",
        "    undo_btn=\"Undo ‚Ü©\",\n",
        "    clear_btn=\"Clear üóëÔ∏è\"\n",
        ")\n",
        "\n",
        "# Launch the Web UI\n",
        "demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_title"
      },
      "source": [
        "### Congratulations!\n",
        "\n",
        "You've successfully built a fully robust RAG pipeline incorporating state-of-the-art technology:\n",
        "- **LangChain** for chaining logical blocks.\n",
        "- **FAISS** alongside Dense Embeddings for high-speed retrieval.\n",
        "- **4-Bit Quantized Models** (`TinyLlama`) executing LLM logic locally and quickly.\n",
        "- **Gradio** for serving a beautiful front-end.\n",
        "\n",
        "**Challenge**: Try returning to **Step 1**, grab a different URL (like a Wikipedia article on Quantum Computing or the history of ancient Rome), reset the runtime, and execute all the cells again to change your Chatbot's Knowledge Base!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
