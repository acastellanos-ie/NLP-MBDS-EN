{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Implementing a Step-by-Step RAG Practice with LangChain\n",
        "\n",
        "Welcome to this interactive notebook where we will build a **Retrieval-Augmented Generation (RAG)** system! \n",
        "\n",
        "In previous practices, we explored Extractive Question Answering and standalone Large Language Models (LLMs) like LLaMA-2 acting as chatbots. While powerful, LLMs have a crucial limitation: they are prone to **hallucinations** (inventing facts) and lack access to private or recent data not seen during their training.\n",
        "\n",
        "**RAG** solves this issue by combining two components:\n",
        "1.  **Retrieval Component**: Searches a custom knowledge base (like your own PDF documents, databases, or websites) for relevant information based on a user's question.\n",
        "2.  **Generation Component**: A powerful LLM takes the retrieved information as \"context\" and uses it to formulate a precise, well-reasoned answer.\n",
        "\n",
        "To build this efficiently, we will use **[LangChain](https://python.langchain.com/)**, a state-of-the-art framework designed specifically to make building applications powered by LLMs a breeze.\n",
        "\n",
        "### In this notebook, we will:\n",
        "- Set up the environment and install necessary libraries.\n",
        "- **Step 1**: Load and chunk a custom document to create our Knowledge Base.\n",
        "- **Step 2**: Create vector Embeddings and a Vector Store (FAISS) for lightning-fast retrieval.\n",
        "- **Step 3**: Initialize an efficient generative LLM using 4-bit Quantization (to run fast on free hardware).\n",
        "- **Step 4**: Assemble the RetrievalQA Chain using LangChain.\n",
        "- **Step 5**: Map our fully functional RAG app to a beautiful interactive Web UI using Gradio!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu_reminder"
      },
      "source": [
        "Ensure that you have the **GPU runtime** activated:\n",
        "(Runtime -> Change runtime type -> Hardware accelerator -> GPU (T4 is perfect))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_title"
      },
      "source": [
        "## Setup: Installing Dependencies\n",
        "\n",
        "Let's install all the specialized tools we need. This includes LangChain components, HuggingFace transformers, FAISS (vector DB), and Gradio (UI).\n",
        "\n",
        "*Note: We are installing `bitsandbytes` and `accelerate` to load the LLM efficiently using quantization.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_code"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq langchain langchain-community langchain-huggingface\n",
        "!pip install -Uqqq sentence-transformers faiss-cpu beautifulsoup4\n",
        "!pip install -Uqqq transformers accelerate bitsandbytes\n",
        "!pip install -Uqqq gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_title"
      },
      "source": [
        "## Step 1: Document Loading and Chunking\n",
        "\n",
        "To build our custom knowledge base, we need a document. For this example, let's scrape a Wikipedia article using LangChain's handy `WebBaseLoader`.\n",
        "\n",
        "However, LLMs have a **context window limit** (e.g., they can only process 2000 words at a time). To solve this, we must split our long document into smaller, manageable pieces called **chunks**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step1_code"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 1. Load the document (You can change this URL to any article you like!)\n",
        "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "loader = WebBaseLoader(url)\n",
        "data = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(data)} document(s).\")\n",
        "print(f\"Original character count: {len(data[0].page_content)}\")\n",
        "\n",
        "# 2. Split the document into chunks\n",
        "# We use RecursiveCharacterTextSplitter which tries to keep paragraphs and sentences together.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,   # Maximum size of each chunk\n",
        "    chunk_overlap=150, # Overlap helps prevent cutting the context mid-sentence\n",
        "    add_start_index=True\n",
        ")\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "print(f\"\\nSplit into {len(docs)} chunks.\")\n",
        "print(f\"Example Chunk:\\n{docs[10].page_content[:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_title"
      },
      "source": [
        "## Step 2: Embeddings and Vector Store (The Retriever)\n",
        "\n",
        "Now we have our text chunks. How do we quickly search through them when a user asks a question?\n",
        "\n",
        "We use an **Embedding Model** to convert text into fixed-size numbers (vectors). Texts with similar meanings end up as vectors pointing in the same direction. We store these vectors in a **Vector Store** (like FAISS) so we can run blazing fast \"similarity searches\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step2_code"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# 1. Select the embedding model\n",
        "# all-MiniLM-L6-v2 is an excellent, compact embedding model built by SentenceTransformers\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# 2. Create the FAISS Vector Index\n",
        "# This processes all 'docs' through the embedding model and builds the searchable database\n",
        "print(\"Generating embeddings and indexing into FAISS. This may take a minute...\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Create the Retrieval interface\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Retrieve the top 3 most relevant chunks\n",
        "print(\"Indexing Complete!\")\n",
        "\n",
        "# Let's test the retriever standalone!\n",
        "test_query = \"What is machine learning?\"\n",
        "relevant_docs = retriever.invoke(test_query)\n",
        "print(f\"\\nRetrieved {len(relevant_docs)} docs for the query '{test_query}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_title"
      },
      "source": [
        "## Step 3: Generator Setup (The LLM)\n",
        "\n",
        "This is the brain that will formulate the final answer. \n",
        "Instead of requiring you to accept usage policies for private models, we will use a fantastic, robust open model: **`TinyLlama/TinyLlama-1.1B-Chat-v1.0`**.\n",
        "Despite its 'Tiny' name, it's very competent for instruction-following.\n",
        "\n",
        "To make it ultra-fast and memory-friendly in Colab, we load it in **4-bit precision** using the `bitsandbytes` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step3_code"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Configuration for 4-bit Quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(f\"Loading model tokenizer and weights ({model_id})...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\" # Automatically maps to GPU if available\n",
        ")\n",
        "\n",
        "# Build the HuggingFace Generation Pipeline\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=0.1,    # We keep temperature low in RAG to avoid hallucinations\n",
        "    max_new_tokens=256, # Max length of the answer it generates\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False # We only want the generated answer, not the prompt echoed back\n",
        ")\n",
        "\n",
        "# Wrap the pipeline so LangChain can converse with it\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "print(\"LLM loaded and pipeline wrapped!\")"
      ]
    },    
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_title"
      },
      "source": [
        "## Step 4: Putting It All Together (The RAG Chain)\n",
        "\n",
        "We have our Retriever (FAISS) and our Generator (TinyLlama). Now we use LangChain to wire them together.\n",
        "\n",
        "We'll define a **Prompt Template** that instructs the LLM:\n",
        "\"Here is some context. Use it to answer the question. If you don't know the answer, just say you don't know.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step4_code"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# 1. Define the Prompt explicitly for our Chat Model\n",
        "# Note: This template format <|system|>, <|user|> is specific to TinyLlama-Chat.\n",
        "prompt_template = \"\"\"<|system|>\n",
        "You are an intelligent assistant. Use the following contextual information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context:\n",
        "{context}</s>\n",
        "<|user|>\n",
        "{input}</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"input\"]\n",
        ")\n",
        "\n",
        "# 2. Build the 'Stuff Document' Chain (takes all documents and 'stuffs' them into the prompt placeholder)\n",
        "document_chain = create_stuff_documents_chain(\n",
        "    llm, \n",
        "    prompt\n",
        ")\n",
        "\n",
        "# 3. Build the Retrieval Chain (Wires Retriever + Document Chain together)\n",
        "rag_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "print(\"RAG Pipeline is ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4.5_title"
      },
      "source": [
        "Let's test the RAG Chain programmatically to see if everything works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step4.5_code"
      },
      "outputs": [],
      "source": [
        "user_question = \"Who formulated the concept of weak AI and strong AI?\"\n",
        "\n",
        "# The chain expects the question to be passed via the 'input' key\n",
        "result = rag_chain.invoke({\"input\": user_question})\n",
        "\n",
        "print(\"QUESTION:\", user_question)\n",
        "print(\"\\n--- LLM ANSWER ---\")\n",
        "print(result[\"answer\"])\n",
        "print(\"\\n--- CITED SOURCES (Context) ---\")\n",
        "for i, doc in enumerate(result['context'], 1):\n",
        "    print(f\"Source {i} snippet: {doc.page_content[:150]}...\")"
      ]
    },    
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_title"
      },
      "source": [
        "## Step 5: Interactive Chat UI with Gradio\n",
        "\n",
        "Testing with Python output is great for developers, but applications are built for end-users. We'll wrap our LangChain logic in a `Gradio` Web UI.\n",
        "\n",
        "We define a helper function (`chat_with_rag`) that Gradio will trigger every time the user clicks submit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step5_code"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_with_rag(message, history):\n",
        "    # Using our rag_chain to generate a response\n",
        "    response = rag_chain.invoke({\"input\": message})\n",
        "    \n",
        "    # We fetch the answer string from the output dictionary\n",
        "    answer = response[\"answer\"]\n",
        "    return answer.strip()\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_rag,\n",
        "    title=\"My First RAG App ðŸš€\",\n",
        "    description=\"Ask me anything about Artificial Intelligence! My knowledge is powered by our FAISS index and TinyLlama.\",\n",
        "    examples=[\"What is the Turing test?\", \"Who are the pioneers of AI?\", \"Explain deep learning briefly.\"],\n",
        ")\n",
        "\n",
        "# Launch the Web UI\n",
        "demo.launch(debug=True, share=True) # share=True gives us a nice public link!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_title"
      },
      "source": [
        "### Congratulations!\n",
        "\n",
        "You've successfully built a fully robust RAG pipeline incorporating state-of-the-art technology:\n",
        "- **LangChain** for chaining logical blocks.\n",
        "- **FAISS** alongside Dense Embeddings for high-speed retrieval.\n",
        "- **4-Bit Quantized Models** (`TinyLlama`) executing LLM logic locally and quickly.\n",
        "- **Gradio** for serving a beautiful front-end.\n",
        "\n",
        "**Challenge**: Try returning to **Step 1**, grab a different URL (like a Wikipedia article on Quantum Computing or the history of ancient Rome), reset the runtime, and execute all the cells again to change your Chatbot's Knowledge Base!"
      ]
    }
  ]
}
