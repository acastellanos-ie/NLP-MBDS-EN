{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cb996c7e83b4fe9b5102765c0d9f39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f73fa24c8ee4fedaa0bbfcb780f2ad0",
              "IPY_MODEL_81f0c7eea3bf4f04a1469134e64a29f1",
              "IPY_MODEL_a1a896e6479c482b95309827d6cd2bf1"
            ],
            "layout": "IPY_MODEL_be87e643453e4d22b087bae566cc80e8"
          }
        },
        "7f73fa24c8ee4fedaa0bbfcb780f2ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d242a5294704a4fb52478089bca0dc4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b9318a41c3e14898ab73de1bc73f2eea",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        },
        "81f0c7eea3bf4f04a1469134e64a29f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c82e97c4df244d9faeda3236cdec47ef",
            "max": 201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08f615269e794945901ed6bf9e096721",
            "value": 201
          }
        },
        "a1a896e6479c482b95309827d6cd2bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_982c40a15a4c43699c7d10ff3334270d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_220f0d2a6d77408f9c659de6d409bc48",
            "value": "â€‡201/201â€‡[00:05&lt;00:00,â€‡178.75it/s,â€‡Materializingâ€‡param=model.norm.weight]"
          }
        },
        "be87e643453e4d22b087bae566cc80e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d242a5294704a4fb52478089bca0dc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9318a41c3e14898ab73de1bc73f2eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c82e97c4df244d9faeda3236cdec47ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08f615269e794945901ed6bf9e096721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "982c40a15a4c43699c7d10ff3334270d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "220f0d2a6d77408f9c659de6d409bc48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/NLP-MBDS-EN/blob/main/07_rag/RAG_practice_step_by_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Implementing a Step-by-Step RAG Practice with LangChain\n",
        "\n",
        "Welcome to this interactive notebook where we will build a **Retrieval-Augmented Generation (RAG)** system!\n",
        "\n",
        "In previous practices, we explored Extractive Question Answering and standalone Large Language Models (LLMs) like LLaMA-2 acting as chatbots. While powerful, LLMs have a crucial limitation: they are prone to **hallucinations** (inventing facts) and lack access to private or recent data not seen during their training.\n",
        "\n",
        "**RAG** solves this issue by combining two components:\n",
        "1.  **Retrieval Component**: Searches a custom knowledge base (like your own PDF documents, databases, or websites) for relevant information based on a user's question.\n",
        "2.  **Generation Component**: A powerful LLM takes the retrieved information as \"context\" and uses it to formulate a precise, well-reasoned answer.\n",
        "\n",
        "To build this efficiently, we will use **[LangChain](https://python.langchain.com/)**, a state-of-the-art framework designed specifically to make building applications powered by LLMs a breeze.\n",
        "\n",
        "### In this notebook, we will:\n",
        "- Set up the environment and install necessary libraries.\n",
        "- **Step 1**: Load and chunk a custom document to create our Knowledge Base.\n",
        "- **Step 2**: Create vector Embeddings and a Vector Store (FAISS) for lightning-fast retrieval.\n",
        "- **Step 3**: Initialize an efficient generative LLM using 4-bit Quantization (to run fast on free hardware).\n",
        "- **Step 4**: Assemble the RetrievalQA Chain using LangChain.\n",
        "- **Step 5**: Map our fully functional RAG app to a beautiful interactive Web UI using Gradio!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu_reminder"
      },
      "source": [
        "Ensure that you have the **GPU runtime** activated:\n",
        "(Runtime -> Change runtime type -> Hardware accelerator -> GPU (T4 is perfect))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_title"
      },
      "source": [
        "## Setup: Installing Dependencies\n",
        "\n",
        "Let's install all the specialized tools we need. This includes LangChain components, HuggingFace transformers, FAISS (vector DB), and Gradio (UI).\n",
        "\n",
        "*Note: We are installing `bitsandbytes` and `accelerate` to load the LLM efficiently using quantization.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_code"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq langchain langchain-community langchain-huggingface langchain-text-splitters\n",
        "!pip install -Uqqq sentence-transformers faiss-cpu beautifulsoup4\n",
        "!pip install -Uqqq transformers accelerate bitsandbytes\n",
        "!pip install -Uqqq gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_title"
      },
      "source": [
        "## Step 1: Document Loading and Chunking\n",
        "\n",
        "To build our custom knowledge base, we need a document. For this example, let's scrape a Wikipedia article using LangChain's handy `WebBaseLoader`.\n",
        "\n",
        "However, LLMs have a **context window limit** (e.g., they can only process 2000 words at a time). To solve this, we must split our long document into smaller, manageable pieces called **chunks**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step1_code"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 1. Load the document (You can change this URL to any article you like!)\n",
        "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "loader = WebBaseLoader(url)\n",
        "data = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(data)} document(s).\")\n",
        "print(f\"Original character count: {len(data[0].page_content)}\")\n",
        "\n",
        "# 2. Split the document into chunks\n",
        "# We use RecursiveCharacterTextSplitter which tries to keep paragraphs and sentences together.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,   # Maximum size of each chunk\n",
        "    chunk_overlap=150, # Overlap helps prevent cutting the context mid-sentence\n",
        "    add_start_index=True\n",
        ")\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "print(f\"\\nSplit into {len(docs)} chunks.\")\n",
        "print(f\"Example Chunk:\\n{docs[10].page_content[:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_title"
      },
      "source": [
        "## Step 2: Embeddings and Vector Store (The Retriever)\n",
        "\n",
        "Now we have our text chunks. How do we quickly search through them when a user asks a question?\n",
        "\n",
        "We use an **Embedding Model** to convert text into fixed-size numbers (vectors). Texts with similar meanings end up as vectors pointing in the same direction. We store these vectors in a **Vector Store** (like FAISS) so we can run blazing fast \"similarity searches\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step2_code"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# 1. Select the embedding model\n",
        "# all-MiniLM-L6-v2 is an excellent, compact embedding model built by SentenceTransformers\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# 2. Create the FAISS Vector Index\n",
        "# This processes all 'docs' through the embedding model and builds the searchable database\n",
        "print(\"Generating embeddings and indexing into FAISS. This may take a minute...\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Create the Retrieval interface\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Retrieve the top 3 most relevant chunks\n",
        "print(\"Indexing Complete!\")\n",
        "\n",
        "# Let's test the retriever standalone!\n",
        "test_query = \"What is machine learning?\"\n",
        "relevant_docs = retriever.invoke(test_query)\n",
        "print(f\"\\nRetrieved {len(relevant_docs)} docs for the query '{test_query}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_title"
      },
      "source": [
        "## Step 3: Generator Setup (The LLM)\n",
        "\n",
        "This is the brain that will formulate the final answer.\n",
        "Instead of requiring you to accept usage policies for private models, we will use a fantastic, robust open model: **`TinyLlama/TinyLlama-1.1B-Chat-v1.0`**.\n",
        "Despite its 'Tiny' name, it's very competent for instruction-following.\n",
        "\n",
        "To make it ultra-fast and memory-friendly in Colab, we load it in **4-bit precision** using the `bitsandbytes` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "step3_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "0cb996c7e83b4fe9b5102765c0d9f39f",
            "7f73fa24c8ee4fedaa0bbfcb780f2ad0",
            "81f0c7eea3bf4f04a1469134e64a29f1",
            "a1a896e6479c482b95309827d6cd2bf1",
            "be87e643453e4d22b087bae566cc80e8",
            "0d242a5294704a4fb52478089bca0dc4",
            "b9318a41c3e14898ab73de1bc73f2eea",
            "c82e97c4df244d9faeda3236cdec47ef",
            "08f615269e794945901ed6bf9e096721",
            "982c40a15a4c43699c7d10ff3334270d",
            "220f0d2a6d77408f9c659de6d409bc48"
          ]
        },
        "outputId": "63334681-ee46-4cff-d034-107ab5aaa96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model tokenizer and weights (TinyLlama/TinyLlama-1.1B-Chat-v1.0)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cb996c7e83b4fe9b5102765c0d9f39f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'temperature', 'max_new_tokens', 'repetition_penalty'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM loaded and pipeline wrapped!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Configuration for 4-bit Quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(f\"Loading model tokenizer and weights ({model_id})...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\" # Automatically maps to GPU if available\n",
        ")\n",
        "\n",
        "# Build the HuggingFace Generation Pipeline\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=0.1,    # We keep temperature low in RAG to avoid hallucinations\n",
        "    max_new_tokens=256, # Max length of the answer it generates\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False # We only want the generated answer, not the prompt echoed back\n",
        ")\n",
        "\n",
        "# Wrap the pipeline so LangChain can converse with it\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "print(\"LLM loaded and pipeline wrapped!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_title"
      },
      "source": [
        "## Step 4: Putting It All Together (The RAG Chain)\n",
        "\n",
        "We have our Retriever (FAISS) and our Generator (TinyLlama). Now we use LangChain to wire them together.\n",
        "\n",
        "We'll define a **Prompt Template** that instructs the LLM:\n",
        "\"Here is some context. Use it to answer the question. If you don't know the answer, just say you don't know.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "step4_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f29ef68-512c-41fc-b9cd-bdc57494f5c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Pipeline is ready!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. Define the Prompt explicitly for our Chat Model\n",
        "# Note: This template format <|system|>, <|user|> is specific to TinyLlama-Chat.\n",
        "prompt_template = \"\"\"<|system|>\n",
        "You are an intelligent assistant. Use the following contextual information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context:\n",
        "{context}</s>\n",
        "<|user|>\n",
        "{input}</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template_str)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# 2. Build the Retrieval Chain (Wires Retriever + Document Chain together)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG Pipeline is ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4.5_title"
      },
      "source": [
        "Let's test the RAG Chain programmatically to see if everything works:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# Asumimos que prompt, llm, retriever y format_docs ya existen del paso anterior.\n",
        "\n",
        "# Paso A: Definir la rama de generaciÃ³n de respuesta\n",
        "# Esta sub-cadena toma el diccionario con documentos crudos y pregunta,\n",
        "# formatea los docs a string, y genera la respuesta.\n",
        "answer_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=lambda x: format_docs(x[\"context\"])  # Formateamos docs a string SOLO para el LLM\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Paso B: Construir la cadena principal que devuelve todo\n",
        "# 1. 'context': recupera documentos (los guarda como objetos)\n",
        "# 2. 'input': pasa la pregunta\n",
        "# 3. .assign(answer=...): aÃ±ade la clave 'answer' calculada por la answer_chain\n",
        "rag_chain_with_sources = (\n",
        "    RunnableParallel(\n",
        "        {\"context\": itemgetter(\"input\") | retriever, \"input\": itemgetter(\"input\")}\n",
        "    )\n",
        "    .assign(answer=answer_chain)\n",
        ")"
      ],
      "metadata": {
        "id": "n89bzuPDXvg8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "step4.5_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9819c14-c30b-4b0a-e9e4-064e67531258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION: Who formulated the concept of weak AI and strong AI?\n",
            "\n",
            "--- LLM ANSWER ---\n",
            "The concept of weak AI and strong AI was first proposed by the philosopher John Searle in his book \"Dreams of the Self\" published in 1999. Searle argued that while machines could simulate human thought processes, they would not be able to truly understand or have other cognitive states. He coined the term \"weak AI\" to describe machines that were not capable of fully understanding or having other cognitive states. Searle's definition of \"strong AI\" was later refined by the philosopher David Chalmers in his book \"Consciousness and the Nature of Existence\" published in 2003. Chalmers defined strong AI as \"robots that can perform tasks that humans cannot, including tasks that require them to think about things like time, space, and causality.\"\n",
            "\n",
            "--- CITED SOURCES (Context) ---\n",
            "Source 1 snippet: ^  Searle presented this definition of \"Strong AI\" in 1999.[430] Searle's original formulation was \"The appropriately programmed computer really is a ...\n",
            "Source 2 snippet: History of AI  Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBNÂ 0-465-02997-3. McCorduck,...\n",
            "Source 3 snippet: ^ Russell and Norvig write: \"in almost all cases, these early systems failed on more difficult problems\"[372]  ^  Embodied approaches to AI[379] were ...\n"
          ]
        }
      ],
      "source": [
        "user_question = \"Who formulated the concept of weak AI and strong AI?\"\n",
        "\n",
        "# Invocamos pasando un diccionario, como lo configuramos con itemgetter('input')\n",
        "result = rag_chain_with_sources.invoke({\"input\": user_question})\n",
        "\n",
        "print(\"QUESTION:\", user_question)\n",
        "print(\"\\n--- LLM ANSWER ---\")\n",
        "# En LCEL puro, la respuesta suele estar directamente en el output si no usas .assign,\n",
        "# pero con nuestra estructura nueva, 'answer' es una clave del diccionario.\n",
        "print(result[\"answer\"])\n",
        "\n",
        "print(\"\\n--- CITED SOURCES (Context) ---\")\n",
        "# 'result['context']' ahora contiene la lista de Documentos originales\n",
        "for i, doc in enumerate(result['context'], 1):\n",
        "    # .page_content es el atributo estÃ¡ndar de LangChain\n",
        "    content_preview = doc.page_content.replace(\"\\n\", \" \")[:150]\n",
        "    print(f\"Source {i} snippet: {content_preview}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_title"
      },
      "source": [
        "## Step 5: Interactive Chat UI with Gradio\n",
        "\n",
        "Testing with Python output is great for developers, but applications are built for end-users. We'll wrap our LangChain logic in a `Gradio` Web UI.\n",
        "\n",
        "We define a helper function (`chat_with_rag`) that Gradio will trigger every time the user clicks submit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "step5_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecae4400-f43f-446a-c546-bd037e2bea5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f57ebfa4ad3a9fdd4c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f57ebfa4ad3a9fdd4c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 766, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 355, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2157, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1632, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 1005, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 544, in __wrapper\n",
            "    return await submit_fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 905, in _submit_fn\n",
            "    response = await run_sync(self.fn, *inputs, limiter=self.limiter)  # type: ignore\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 63, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2502, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4253461498.py\", line 5, in chat_with_rag\n",
            "    response = rag_chain.invoke({\"input\": message})\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3155, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3882, in invoke\n",
            "    key: future.result()\n",
            "         ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3865, in _invoke_step\n",
            "    return context.run(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3155, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/retrievers.py\", line 222, in invoke\n",
            "    result = self._get_relevant_documents(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/vectorstores/base.py\", line 1045, in _get_relevant_documents\n",
            "    docs = self.vectorstore.similarity_search(query, **kwargs_)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_community/vectorstores/faiss.py\", line 643, in similarity_search\n",
            "    docs_and_scores = self.similarity_search_with_score(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_community/vectorstores/faiss.py\", line 515, in similarity_search_with_score\n",
            "    embedding = self._embed_query(query)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_community/vectorstores/faiss.py\", line 266, in _embed_query\n",
            "    return self.embedding_function.embed_query(text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_huggingface/embeddings/huggingface.py\", line 172, in embed_query\n",
            "    return self._embed([text], embed_kwargs)[0]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_huggingface/embeddings/huggingface.py\", line 124, in _embed\n",
            "    texts = [x.replace(\"\\n\", \" \") for x in texts]\n",
            "             ^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'replace'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 766, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 355, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2157, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1632, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 1005, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 544, in __wrapper\n",
            "    return await submit_fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 905, in _submit_fn\n",
            "    response = await run_sync(self.fn, *inputs, limiter=self.limiter)  # type: ignore\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 63, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2502, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4253461498.py\", line 5, in chat_with_rag\n",
            "    response = rag_chain.invoke({\"input\": message})\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3155, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3882, in invoke\n",
            "    key: future.result()\n",
            "         ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3865, in _invoke_step\n",
            "    return context.run(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3155, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/retrievers.py\", line 222, in invoke\n",
            "    result = self._get_relevant_documents(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/vectorstores/base.py\", line 1045, in _get_relevant_documents\n",
            "    docs = self.vectorstore.similarity_search(query, **kwargs_)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_community/vectorstores/faiss.py\", line 643, in similarity_search\n",
            "    docs_and_scores = self.similarity_search_with_score(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_community/vectorstores/faiss.py\", line 515, in similarity_search_with_score\n",
            "    embedding = self._embed_query(query)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_community/vectorstores/faiss.py\", line 266, in _embed_query\n",
            "    return self.embedding_function.embed_query(text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_huggingface/embeddings/huggingface.py\", line 172, in embed_query\n",
            "    return self._embed([text], embed_kwargs)[0]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_huggingface/embeddings/huggingface.py\", line 124, in _embed\n",
            "    texts = [x.replace(\"\\n\", \" \") for x in texts]\n",
            "             ^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'replace'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f57ebfa4ad3a9fdd4c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_with_rag(message, history):\n",
        "    # Using our rag_chain to generate a response\n",
        "    response = rag_chain_with_sources.invoke({\"input\": message})\n",
        "\n",
        "    # We fetch the answer string from the output dictionary\n",
        "    answer = response[\"answer\"]\n",
        "    return answer.strip()\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_rag,\n",
        "    title=\"My First RAG App ðŸš€\",\n",
        "    description=\"Ask me anything about Artificial Intelligence! My knowledge is powered by our FAISS index and TinyLlama.\",\n",
        "    examples=[\"What is the Turing test?\", \"Who are the pioneers of AI?\", \"Explain deep learning briefly.\"],\n",
        ")\n",
        "\n",
        "# Launch the Web UI\n",
        "demo.launch(debug=True, share=True) # share=True gives us a nice public link!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_title"
      },
      "source": [
        "### Congratulations!\n",
        "\n",
        "You've successfully built a fully robust RAG pipeline incorporating state-of-the-art technology:\n",
        "- **LangChain** for chaining logical blocks.\n",
        "- **FAISS** alongside Dense Embeddings for high-speed retrieval.\n",
        "- **4-Bit Quantized Models** (`TinyLlama`) executing LLM logic locally and quickly.\n",
        "- **Gradio** for serving a beautiful front-end.\n",
        "\n",
        "**Challenge**: Try returning to **Step 1**, grab a different URL (like a Wikipedia article on Quantum Computing or the history of ancient Rome), reset the runtime, and execute all the cells again to change your Chatbot's Knowledge Base!"
      ]
    }
  ]
}