{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/NLP-MBDS-EN/blob/main/05_language_modeling/language_modeling_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Modeling (Decoder-Only Architecture)\n",
        "\n",
        "**Learning Objective:**\n",
        "Dismantle the illusion of \"Generative AI\". A Large Language Model does not \"think\" or \"write\" paragraphs. Its only physical capability is calculating the statistical probability distribution of the *very next token*.\n",
        "\n",
        "In this lab, we will open the black box. We will bypass the high-level `generate()` APIs and manually build an Auto-regressive Generation loop from scratch using raw mathematical logits."
      ],
      "metadata": {
        "id": "SgSuH_hKTMCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Environment Setup\n",
        "!pip install -q transformers torch matplotlib numpy"
      ],
      "metadata": {
        "id": "DoUDE5NaTMCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 1: The Engine (Loading a Decoder)\n",
        "We will use GPT-2. While older, its core architecture is identical to GPT-4 or Llama-3: it is a Transformer Decoder trained strictly on Next-Token Prediction."
      ],
      "metadata": {
        "id": "Ds5jECy3TMCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load Model and Tokenizer\n",
        "model_id = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
        "\n",
        "# Set to evaluation mode (turns off dropout layers for deterministic outputs)\n",
        "model.eval()\n",
        "print(f\"{model_id} loaded successfully. Vocabulary Size: {tokenizer.vocab_size}\")"
      ],
      "metadata": {
        "id": "poBLfLc6TMCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 2: First Principles - The Probability Distribution\n",
        "Let's feed the model an unfinished sentence. We don't want it to generate text yet. We want to see the raw scores (Logits) it assigns to all 50,257 words in its vocabulary for the next position."
      ],
      "metadata": {
        "id": "hIgboJabTMCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The name of the capital of France is\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass without calculating gradients (saves memory)\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "\n",
        "# The model returns logits for EVERY token in the sequence.\n",
        "# We only care about the predictions for the VERY LAST token.\n",
        "next_token_logits = outputs.logits[0, -1, :]\n",
        "\n",
        "# Convert raw logits into probabilities (0 to 1) using Softmax\n",
        "probabilities = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "# Find the top 5 most likely next tokens\n",
        "top_k = 5\n",
        "top_probs, top_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "# Decode indices back to human-readable text\n",
        "top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
        "\n",
        "# --- Visualization ---\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.barh(top_tokens[::-1], top_probs.numpy()[::-1] * 100, color='skyblue')\n",
        "plt.xlabel(\"Probability (%)\")\n",
        "plt.title(f\"Next Token Distribution for: '{prompt}'\")\n",
        "plt.xlim(0, 100)\n",
        "for i, v in enumerate(top_probs.numpy()[::-1] * 100):\n",
        "    plt.text(v + 1, i, f\"{v:.2f}%\", va='center')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_PP13OxyTMCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TAKEAWAY:\n",
        "\n",
        "Notice that `Paris` has a high probability.\n",
        "\n",
        "The model \"knows\" the answer not because it has a database, but because in its training data, `Paris` statistically followed those exact previous words highly often."
      ],
      "metadata": {
        "id": "4SbP37aAYLkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 3: Building Auto-Regressive Generation from Scratch\n",
        "Now we will build the engine.\n",
        "1. Predict the next token.\n",
        "2. Take the most probable token (Greedy Decoding) and append it to our prompt.\n",
        "3. Feed the new, longer prompt back into the model.\n",
        "4. Repeat."
      ],
      "metadata": {
        "id": "H2WGSB5DTMCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_from_scratch(prompt, max_new_tokens=15, penalty=1.2):\n",
        "    current_input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    print(f\"\\nStarting Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for step in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(current_input_ids)\n",
        "\n",
        "        next_token_logits = outputs.logits[0, -1, :]\n",
        "\n",
        "        # SIMPLE REPETITION PENALTY:\n",
        "        # Reduce the score of tokens that have already appeared in the sequence\n",
        "        for token_id in current_input_ids[0]:\n",
        "            if next_token_logits[token_id] > 0:\n",
        "                next_token_logits[token_id] /= penalty\n",
        "            else:\n",
        "                next_token_logits[token_id] *= penalty\n",
        "\n",
        "        next_token_id = torch.argmax(next_token_logits)\n",
        "        next_word = tokenizer.decode([next_token_id])\n",
        "        print(f\"Step {step+1}: '{next_word}'\")\n",
        "\n",
        "        current_input_ids = torch.cat([current_input_ids, next_token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
        "\n",
        "    final_text = tokenizer.decode(current_input_ids[0])\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Final Output:\\n{final_text}\")\n",
        "\n",
        "# Testing with penalty to avoid the loop\n",
        "generate_from_scratch(\"The name of the capital of France is\")"
      ],
      "metadata": {
        "id": "3g1w0XnsTMCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 4: The Problem with Greedy Decoding\n",
        "If we ALWAYS pick the most mathematically probable token (Argmax), the model becomes boring and often gets stuck in repetitive loops.\n",
        "\n",
        "This is why the **Temperature** parameter was invented. Instead of picking the absolute top token, we \"sample\" from the probability distribution. Higher temperature flattens the distribution, allowing \"weaker\" words to sometimes be chosen, increasing creativity (or hallucination)."
      ],
      "metadata": {
        "id": "SZUuv-rETMCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the built-in generate API with extreme temperature differences\n",
        "prompt = \"The name of the capital of France is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Low Temperature (Very predictable, high repetition risk)\n",
        "out_low = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=20,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    repetition_penalty=1.1,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# High Temperature (Chaotic, high 'creativity')\n",
        "out_high = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=20,\n",
        "    do_sample=True,\n",
        "    temperature=1.5,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"--- LOW TEMPERATURE (0.1) + PENALTY ---\")\n",
        "print(tokenizer.decode(out_low[0], skip_special_tokens=True))\n",
        "\n",
        "print(\"\\n--- HIGH TEMPERATURE (1.5) ---\")\n",
        "print(tokenizer.decode(out_high[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "yOVAapOxTMCO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}