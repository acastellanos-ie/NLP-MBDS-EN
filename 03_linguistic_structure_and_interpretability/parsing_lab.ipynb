{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/NLP-MBDS-EN/blob/main/03_linguistic_structure_and_interpretability/parsing_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classical Syntax and its Limitations (Lab)\n",
        "\n",
        "**Learning Objective:**\n",
        "Understand how classical NLP uses rule-based Dependency Parsing to extract structured information (Subject-Verb-Object) from text.\n",
        "\n",
        "More importantly, we will empirically expose the fragility of rule-based systems when faced with real-world linguistic variability, setting the stage for the deep learning revolution (Semantics & Transformers)."
      ],
      "metadata": {
        "id": "eosnrbIbEpQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Environment Setup\n",
        "# We use spaCy, the industry standard library for classical NLP pipelines.\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm -q"
      ],
      "metadata": {
        "id": "prBlpSiqEpQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 1: Visualizing Linguistic Structure\n",
        "Before deep learning, we relied on parsing sentences into directed graphs (Dependency Trees) to understand grammatical relationships."
      ],
      "metadata": {
        "id": "tTxfRm7KEpQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "# Load the small English pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Render the dependency tree using displaCy\n",
        "html = displacy.render(doc, style=\"dep\", jupyter=False, options={'distance': 100})\n",
        "display(HTML(html))"
      ],
      "metadata": {
        "id": "frCvnBvDEpQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 2: The Illusion of Rule-Based Information Extraction\n",
        "Let's build a classic Subject-Verb-Object (SVO) extractor. This was the foundation of early Knowledge Graphs and Question Answering (QA) systems."
      ],
      "metadata": {
        "id": "t_yt9-tBEpQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_svo_triples(text, nlp_model):\n",
        "    doc = nlp_model(text)\n",
        "    svo_triples = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Find the Subject\n",
        "        if \"subj\" in token.dep_:\n",
        "            subject = token\n",
        "            verb = token.head\n",
        "            # Find the Object attached to the Verb\n",
        "            for obj in verb.children:\n",
        "                if \"obj\" in obj.dep_:\n",
        "                    svo_triples.append((subject, verb, obj))\n",
        "                elif obj.dep_ == \"prep\":\n",
        "                    for pobj in obj.children:\n",
        "                        if pobj.dep_ == \"pobj\":\n",
        "                            svo_triples.append((subject, verb, pobj))\n",
        "    return svo_triples\n",
        "\n",
        "# Let's test it on a sterile, simple dataset\n",
        "sterile_text = \"John bought a new car. Mary gave John a book. Alice traveled to Paris.\"\n",
        "sterile_triples = extract_svo_triples(sterile_text, nlp)\n",
        "\n",
        "print(\"--- Extracted Triples ---\")\n",
        "for triple in sterile_triples:\n",
        "    print(f\"Subject: {triple[0].text: <5} | Verb: {triple[1].text: <8} | Object: {triple[2].text}\")"
      ],
      "metadata": {
        "id": "askXf4l1EpQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 3: Building a Simple QA System\n",
        "We can use these triples to answer basic questions by matching the verb."
      ],
      "metadata": {
        "id": "ROzbkedjEpQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_qa(question, svo_triples, nlp_model):\n",
        "    question_doc = nlp_model(question)\n",
        "    question_verb = None\n",
        "\n",
        "    # Identify the main verb in the question\n",
        "    for token in question_doc:\n",
        "        if \"VERB\" in token.pos_:\n",
        "            question_verb = token\n",
        "            break\n",
        "\n",
        "    if question_verb is not None:\n",
        "        for triple in svo_triples:\n",
        "            subject, verb, obj = triple\n",
        "            # Match by lemma (base form of the verb: bought -> buy)\n",
        "            if verb.lemma_ == question_verb.lemma_:\n",
        "                return f\"{subject.text} {verb.text} {obj.text}\"\n",
        "\n",
        "    return \"System Failure: Cannot determine answer.\"\n",
        "\n",
        "print(\"Question: Who bought a car?\")\n",
        "print(\"Answer:  \", simple_qa(\"Who bought a car?\", sterile_triples, nlp))"
      ],
      "metadata": {
        "id": "2GipQh2dEpQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 4: The Stress Test\n",
        "The system above looks like artificial intelligence. **It is not.** It is a rigid topological rule.\n",
        "\n",
        "Let's see what happens when we introduce standard linguistic variance (Synonyms, Passive Voice, Complex Clauses)."
      ],
      "metadata": {
        "id": "KQAq49qpEpQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The real world does not speak in perfect SVO structures.\n",
        "real_world_text = \"A brand new car was purchased by John. The book, which was heavy, was given to John by Mary.\"\n",
        "real_world_triples = extract_svo_triples(real_world_text, nlp)\n",
        "\n",
        "print(\"--- QA Stress Test ---\")\n",
        "questions = [\n",
        "    \"Who bought a car?\",           # Fails: Synonym (purchased vs bought)\n",
        "    \"What did John purchase?\",     # Fails: Syntactic inversion (Passive voice makes 'car' the subject)\n",
        "    \"Who gave the book to John?\"   # Fails: Passive voice makes 'book' the subject\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {simple_qa(q, real_world_triples, nlp)}\")\n",
        "\n",
        "print(\"\\n--- Why did it fail? Look at the Triples ---\")\n",
        "for triple in real_world_triples:\n",
        "    print(f\"Parsed -> Subject: {triple[0].text: <4} | Verb: {triple[1].text: <9} | Object: {triple[2].text}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OS9kSaqrEpQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TAKEAWAY FOR THE STUDENT:\n",
        "\n",
        "In the passive sentence: `A car was purchased by John` the syntax parser labels \"car\" as the grammatical subject `(nsubjpass)`\n",
        "\n",
        "Our hardcoded logic thinks the car is doing the purchasing.\n",
        "\n",
        "To fix this in classical NLP, you would need hundreds of \"if/else\" rules to handle every grammatical edge case.\n",
        "This is unsustainable.\n",
        "\n",
        "**The solution?**\n",
        "\n",
        "Dense Vector Semantics (Next Session)."
      ],
      "metadata": {
        "id": "F6ZynMTZE9zR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e89VOLntFj2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}