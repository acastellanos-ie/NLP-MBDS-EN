{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/NLP-MBDS-EN/blob/main/08_agentic_ai/agentic_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcBHzV3fMNgX"
      },
      "source": [
        "# üß† Lab Session: Agentic AI with Python Logic & Reasoning\n",
        "\n",
        "**Objective:**\n",
        "In this final session, we build an **Autonomous Agent**. Unlike a chatbot that just talks, an Agent can *do things*. We will give our Llama-2 model access to a **Python Interpreter**. This allows the AI to write code to solve logic puzzles, perform complex math, or process data, effectively giving it the capabilities of a Data Scientist.\n",
        "\n",
        "**The Architecture:**\n",
        "1.  **Brain:** Llama-2-7b (Quantized) decides *what* to do.\n",
        "2.  **Tool A (Memory):** A Vector Database (RAG) for specific facts about our fictional planet.\n",
        "3.  **Tool B (Logic):** A Python REPL (Read-Eval-Print Loop) to execute code.\n",
        "4.  **Interface:** Gradio to visualize the Agent's \"Thought Process\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IofmGfyOMNgl"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "We need `langchain-experimental` to access the Python execution tools safely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcg1H4h6MNgo",
        "outputId": "68ef2140-2d3e-40d0-be17-159883472753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent Environment set up successfully!\n"
          ]
        }
      ],
      "source": [
        "# @title üõ†Ô∏è Install Libraries\n",
        "!pip install -Uqqq langchain-community langchain-huggingface langchain langchain-experimental chromadb sentence-transformers faiss-cpu transformers torch accelerate bitsandbytes gradio huggingface_hub\n",
        "print(\"Agent Environment set up successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-rdEMmJMNgv"
      },
      "source": [
        "## 2. The Brain\n",
        "\n",
        "We load the model in 4-bit mode to fit comfortably in the free Colab GPU. We set `temperature=0.01` because we need the model to be precise when calling tools, not creative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GHxFQ1QMNgw",
        "outputId": "86b74d67-1bfb-44c8-b83a-609fdb6690fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TinyLlama/TinyLlama-1.1B-Chat-v1.0 as the Agent's Brain...\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# 1. 4-Bit Quantization Config (Efficiency First)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(f\"Loading {model_id} as the Agent's Brain...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 2. Text Generation Pipeline\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512, # Agents need space to \"think\"\n",
        "    temperature=0.01,   # Precision over creativity\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "print(\"Agent Brain Ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCn4NGfpMNgy"
      },
      "source": [
        "## 3. Creating the Tools (Hands & Calculator)\n",
        "\n",
        "We will give the Agent two superpowers:\n",
        "1.  **Planetary Knowledge (RAG):** To retrieve facts it wasn't trained on.\n",
        "2.  **Python Interpreter:** To execute code for math and logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGYeslRaMNgz"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.tools import Tool\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "\n",
        "# --- TOOL 1: RAG (Planetary Knowledge) ---\n",
        "planet_data = \"\"\"\n",
        "Proxima Centauri b is a habitable planet orbiting the red dwarf Proxima Centauri.\n",
        "The distance from Earth to Proxima Centauri b is approximately 4.24 light-years.\n",
        "The average surface temperature is -39 degrees Celsius.\n",
        "The orbital period (one year) is exactly 11.2 Earth days.\n",
        "The colonization mission \"Exodus-1\" arrived in 2150.\n",
        "The population is currently 4,500 colonists.\n",
        "The currency is \"Stardust-Credits\".\n",
        "\"\"\"\n",
        "\n",
        "# Vector Store Setup\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents([planet_data])\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_db = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vector_db.as_retriever()\n",
        "\n",
        "knowledge_tool = Tool(\n",
        "    name=\"Planetary Database\",\n",
        "    func=retriever.invoke,\n",
        "    description=\"Useful for answering factual questions about Proxima Centauri b, its distance, population, or history.\"\n",
        ")\n",
        "\n",
        "# --- TOOL 2: PYTHON REPL (Logic & Math) ---\n",
        "python_tool = PythonREPLTool()\n",
        "python_tool.name = \"Python Interpreter\"\n",
        "python_tool.description = \"A Python shell. Use this to execute python commands. Input should be a valid python command. Useful for complex logic, math, sorting lists, or processing text strings.\"\n",
        "\n",
        "# Combine tools\n",
        "tools = [knowledge_tool, python_tool]\n",
        "\n",
        "print(\"Tools created: [Planetary Database, Python Interpreter]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhCp_Dy6MNg1"
      },
      "source": [
        "## 4. Initializing the Agent (The Orchestrator)\n",
        "\n",
        "We use the **ReAct (Reason + Act)** framework. The agent will loop through:\n",
        "* **Thought:** \"What should I do?\"\n",
        "* **Action:** \"Run this Python code.\"\n",
        "* **Observation:** \"Here is the result of the code.\"\n",
        "* **Final Answer:** \"Here is the summary.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D-DNgq6MNg3"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "# Initialize the Agent\n",
        "# 'verbose=True' is essential to see the reasoning steps in the console.\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True # Llama-2 sometimes makes formatting mistakes; this fixes them automatically.\n",
        ")\n",
        "\n",
        "print(\"Agent Initialized. Ready to Reason and Act.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSFngvuMNg6"
      },
      "source": [
        "## 5. Testing the Logic\n",
        "\n",
        "Let's verify that the Python tool is working by asking something Llama-2 cannot do purely by text prediction (like iterating through a sequence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lz3v0lCMNg-"
      },
      "outputs": [],
      "source": [
        "# Test: Fibonacci Sequence\n",
        "# The LLM should write a Python loop to solve this.\n",
        "query = \"Generate the first 10 numbers of the Fibonacci sequence and calculate their sum.\"\n",
        "\n",
        "print(f\"User Query: {query}\")\n",
        "print(\"-\" * 40)\n",
        "result = agent.run(query)\n",
        "print(\"-\" * 40)\n",
        "print(f\"Final Answer: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V88S4HZCMNg_"
      },
      "source": [
        "## 6. The Interface (Gradio)\n",
        "\n",
        "We will build a Chat Interface that reveals the **\"Brain's Monologue\"**. We capture the system logs (stdout) to show the user exactly how the Agent decided to write Python code or check the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tby6_jwMNg_"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import sys\n",
        "from io import StringIO\n",
        "\n",
        "def solve_with_reasoning(message, history):\n",
        "    # 1. Capture the \"Thinking Process\" (Stdout)\n",
        "    old_stdout = sys.stdout\n",
        "    sys.stdout = mystdout = StringIO()\n",
        "\n",
        "    try:\n",
        "        # 2. Run the Agent\n",
        "        result = agent.run(message)\n",
        "    except Exception as e:\n",
        "        result = f\"I encountered an error: {str(e)}\"\n",
        "\n",
        "    # 3. Restore Stdout & Get Logs\n",
        "    sys.stdout = old_stdout\n",
        "    reasoning_logs = mystdout.getvalue()\n",
        "\n",
        "    # 4. Format Output for the UI\n",
        "    # We use Markdown to make the code/logs look like a terminal\n",
        "    formatted_output = (\n",
        "        f\"üß† **Agent Thought Process:**\\n\"\n",
        "        f\"```bash\\n{reasoning_logs}\\n```\\n\\n\"\n",
        "        f\"‚úÖ **Final Answer:**\\n{result}\"\n",
        "    )\n",
        "    return formatted_output\n",
        "\n",
        "# Create the Gradio Interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=solve_with_reasoning,\n",
        "    title=\"ü§ñ Agentic AI: The Python Coder\",\n",
        "    description=\"Ask me complex questions. I can use a Database or write Python code to answer.\",\n",
        "    examples=[\n",
        "        \"What is 30% of the population of Proxima Centauri b?\",\n",
        "        \"Create a list of numbers from 1 to 10, square them, and find the average.\",\n",
        "        \"How many seconds does it take for light to travel from Earth to Proxima b? (Use Python)\",\n",
        "        \"Sort this list alphabetically: [Zebra, Apple, Mango, Delta]\"\n",
        "    ],\n",
        "    theme=\"glass\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}